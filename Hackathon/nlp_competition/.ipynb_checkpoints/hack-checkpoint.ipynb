{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word vectors\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'wv.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-25b887046730>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading word vectors\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#Load wordVectors and wordVecDim from pickle file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mwordVectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordVecDim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wv.pkl'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'wv.pkl'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Setting MODE to 0 trains the model\n",
    "Setting MODE to 1 allows the model to run on user input \n",
    "Setting MODE to 2 creates final 'predictions.json' file\n",
    "'''\n",
    "MODE = 0\n",
    "'''\n",
    "Lets you toggle whether you want to used previously saved\n",
    "weights from the last run or override any previously saved \n",
    "weights.\n",
    "'''\n",
    "USE_OLD = True\n",
    "\n",
    "print(\"Loading word vectors\")\n",
    "#Load wordVectors and wordVecDim from pickle file\n",
    "wordVectors, wordVecDim = pickle.load(open('wv.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Should return the word vector of a word.  If the word is in wordVectors\n",
    "return the appropriate word vector.  If not return a word vector of the\n",
    "same dimension (wordVecDim) but filled with zeros. (Hint: np.zeros)\n",
    "wordVectors[word] will return a wordVecDim dimensional vector\n",
    "if word is in the set of keys in wordVectors.  You can check\n",
    "if word is in the set of keys in wordVectors using \n",
    "'word in wordVectors'.\n",
    "'''\n",
    "def getVector(w):\n",
    "\n",
    "\t#CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Given a string containing words return a list of dimensionality\n",
    "equal to the number of words in 's'.  Each element of the list\n",
    "should be the appropriate word vector for the corresponding word\n",
    "in 's'. Simply put return a list of word vectors of 's'.\n",
    "Remember we can use word_tokenize(string) to partition a given\n",
    "string into a list of words.  Also consider the function you\n",
    "wrote just above.\n",
    "'''\n",
    "def wordVectorize(s):\n",
    "\n",
    "\t#CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODE=0 we need training data, MODE=1 we need no data, MODE=2, we need test data\n",
    "if(MODE==0 or MODE==2):\n",
    "\tprint(\"Loading data...\")\n",
    "\n",
    "\t#Load the training data if training\n",
    "\tif(MODE==0):\n",
    "\t\tlabeledReviews = json.load(open('trainingDataset.json','r'))\n",
    "\n",
    "\t#Load the testing data if testing\n",
    "\tif(MODE==2):\n",
    "\t\ttestReviews = json.load(open('testDataset.json','r'))\n",
    "\n",
    "\t'''\n",
    "\textract accepts reviews: list of dictionaries representing reviews.\n",
    "\tThe dictionaries must hold keys 'title' and 'review_text'.  If ratingsPresent\n",
    "\tthe 'rating' key should also exist\n",
    "\t'''\n",
    "\tdef extract(reviews, ratingsPresent):\n",
    "\n",
    "\t\t#Expression that evaluates to to title of r\n",
    "\t\ttitles = [_______ for r in reviews]\n",
    "\t\t#Expression that evaluates to to review text of r\n",
    "\t\ttexts = [_______ for r in reviews]\n",
    "\t\tif(ratingsPresent):\n",
    "\t\t\t'''\n",
    "\t\t\tExpression that evaluates to 1/True if \n",
    "\t\t\treview is positive and 0/False if review is negative.\n",
    "\t\t\t'''\n",
    "\t\t\tratings = [_______ for r in reviews]\n",
    "\t\t#Expression that evaluates to total number of reviews\n",
    "\t\tnumReviews = ______\n",
    "\n",
    "\t\t#Expression that evaluates to a list of the word vectors of t\n",
    "\t\ttitleVectors = [_______ for t in titles]\n",
    "\t\t#Expression that evaluates to a list of the word vectors of t\n",
    "\t\ttextVectors = [_______ for t in texts]\n",
    "\n",
    "\t\t'''\n",
    "\t\tShould return true if and only if both titleVectors[i] and \n",
    "\t\ttextVectors[i] have positive length.  That is a non zero\n",
    "\t\tnumber of words were found in either.  This is to avoid \n",
    "\t\tnull input errors later.\n",
    "\t\t'''\n",
    "\t\tdef validReview(i):\n",
    "\t\t\t#CODE HERE\n",
    "\n",
    "\n",
    "\t\t'''\n",
    "\t\tGiven a list 'l' where l[i] corresponds to some attribute of\n",
    "\t\tthe ith review, return a list containing all elements of l in\n",
    "\t\torder where the ith review is valid.  Being valid is as defined\n",
    "\t\tin the above function\n",
    "\t\t'''\n",
    "\t\tdef onlyValid(l):\n",
    "\t\t\t#CODE HERE\n",
    "\n",
    "\n",
    "\t\t#Return the appropriate values depending on ratingsPresent\n",
    "\t\tif(ratingsPresent):\n",
    "\t\t\tratings, titleVectors, textVectors = onlyValid(ratings), onlyValid(titleVectors), onlyValid(textVectors)\n",
    "\t\t\treturn ratings, titleVectors, textVectors\n",
    "\t\telse:\n",
    "\t\t\ttitleVectors, textVectors = onlyValid(titleVectors), onlyValid(textVectors)\n",
    "\t\t\treturn titleVectors, textVectors\n",
    "\n",
    "\t#Data preparation for training\n",
    "\tif(MODE==0):\n",
    "\t\t'''\n",
    "\t\tShould be some value between 0 and 1.  Suppose you used 0.4.\n",
    "\t\tThen 40% of the labeled data would be used for training and\n",
    "\t\t60% would be used for validation.  Note that we generally want\n",
    "\t\tto use the majority of the data for training, but there is no\n",
    "\t\tmagic number.\n",
    "\t\t'''\n",
    "\t\ttvCutoff = int(len(labeledReviews)*_____)\n",
    "\t\ttrReviews, valReviews = labeledReviews[:tvCutoff], labeledReviews[tvCutoff:]\n",
    "\n",
    "\t\t'''\n",
    "\t\tGiven a list of reviews return 2 lists. The first contains all\n",
    "\t\tpositive reviews and the latter contains all negative reviews.\n",
    "\t\t'''\n",
    "\t\tdef sentimentSplit(reviews):\n",
    "\t\t\tratings, titles, texts = extract(reviews, ratingsPresent=True)\n",
    "\t\t\tnumValidReviews = len(titles)\n",
    "\n",
    "\t\t\t#Expression that evalutes to true if a rating is positive\n",
    "\t\t\tpositiveReviews = [(titles[i], texts[i], ratings[i]) for i in range(numValidReviews) if _______]\n",
    "\n",
    "\t\t\t#Expression that evalutes to true if a rating is negative\n",
    "\t\t\tnegativeReviews = [(titles[i], texts[i], ratings[i]) for i in range(numValidReviews) if _______]\n",
    "\t\t\t\n",
    "\t\t\treturn positiveReviews, negativeReviews\n",
    "\n",
    "\t\tpositiveTrain, negativeTrain = sentimentSplit(trReviews)\n",
    "\t\tnumPositive = len(positiveTrain)\n",
    "\t\tnumNegative = len(negativeTrain)\n",
    "\n",
    "\t\tvalRatings, valTitles, valTexts = extract(valReviews, ratingsPresent=True)\n",
    "\t\tnumVal = len(valTitles)\n",
    "\n",
    "\t\tvalSet = [(valTitles[i], valTexts[i], valRatings[i]) for i in range(numVal)]\n",
    "\n",
    "\t#Data preparation for testing\n",
    "\tif(MODE==2):\n",
    "\t\tteTitles, teTexts = extract(testReviews, ratingsPresent=False)\n",
    "\t\tnumTest = len(teTitles)\n",
    "\n",
    "\t\ttestSet = [(teTitles[i], teTexts[i]) for i in range(numTest)]\n",
    "\n",
    "print(\"Setting up graph...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "titlePlaceholder will contain the numerical representation of the title\n",
    "of a single review in your computational graph.\n",
    "\n",
    "Search tf.placeholder for how to create a tensorflow placeholder.\n",
    "\n",
    "Use tf.float32 as your datatype.\n",
    "Be careful when specifying the shape attribute.  Remember that\n",
    "titles can be of variable length.  'None' is used to represent\n",
    "a wild card dimension.  For example both [4,2,5] and [4,6,5] would\n",
    "be valid inputs to a placeholder of shape [4,None,5]\n",
    "Name is optional\n",
    "'''\n",
    "titlePlaceholder = ______\n",
    "'''\n",
    "Same as above but for a single review text as opposed to a single\n",
    "title.\n",
    "'''\n",
    "textPlaceholder = _______\n",
    "'''\n",
    "Placeholder for a single float value.  This value should be 1 when\n",
    "the given review is positive and 0 when not.  Note this value is not\n",
    "used in making a prediction only for computing loss while training.\n",
    "'''\n",
    "y = ______\n",
    "\n",
    "'''\n",
    "Dimensionality of the cell/hidden state of your LSTM.\n",
    "Too small and your model won't be sufficiently expressive.\n",
    "Too large and your model will quickly overfit.\n",
    "'''\n",
    "cellSize = ____\n",
    "\n",
    "'''\n",
    "Define your LSTM cell for the title LSTM.\n",
    "\n",
    "Search tf.nn.rnn_cell.LSTMCell to find out how to make one\n",
    "\n",
    "You should only have to supply one parameter, but feel free\n",
    "to modify any other parameters you understand.\n",
    "'''\n",
    "titleLSTM = tf.nn.rnn_cell.LSTMCell(________)\n",
    "\n",
    "'''\n",
    "Define your LSTM that will process the information\n",
    "contained in titlePlaceholder (a single review title)\n",
    "\n",
    "Search tf.nn.dynamic_rnn to find out how to make one.\n",
    "\n",
    "You only have to supply cell, inputs, dtype, and scope.\n",
    "It's up to you to figure out what goes in cell and inputs.\n",
    "Feel free to use tf.float32 as the datatype again.\n",
    "\n",
    "The reason we must specify scope is that the scope name\n",
    "is used to save the parameters of the LSTM.  Since we\n",
    "are going to have another LSTM for the review texts we\n",
    "need to specify different scopes for the two of them so\n",
    "their weights don't collide in storage.\n",
    "\n",
    "a, b are intentionally left ambiguous.  You will have\n",
    "to read in the documentation what they are and figure\n",
    "out what you need.  If you find you only need one of them\n",
    "simply replacing 'a, b' with either '_, b' or 'a, _' will\n",
    "be more efficient as you don't save the unecessary information.\n",
    "'''\n",
    "a, b = tf.nn.dynamic_rnn(______)\n",
    "\n",
    "\n",
    "'''\n",
    "Using a and b (or one of the two) determine the final cell\n",
    "state of the LSTM.  We then reshape it into a column vector\n",
    "of dimensionality cellSize\n",
    "'''\n",
    "titleCellState = tf.reshape(________,[cellSize, 1])\n",
    "\n",
    "'''\n",
    "Repeat the above steps for the textLSTM.\n",
    "Define your LSTM that will process the information\n",
    "contained in textlaceholder (a single review text)\n",
    "'''\n",
    "textLSTM = ________\n",
    "\n",
    "a, b = tf.nn.dynamic_rnn(_____)\n",
    "\n",
    "textCellState = tf.reshape(______,[cellSize, 1])\n",
    "\n",
    "'''\n",
    "Concatenate the two cell states into a single vector of\n",
    "dimensionality [2*cellSize, 1]\n",
    "\n",
    "Search tf.concat for how to do this.\n",
    "'''\n",
    "combinedState = tf.concat(________)\n",
    "\n",
    "'''\n",
    "Returns a tensor variable of the specified size that is\n",
    "initialized using a truncated normal distribution of stddev 0.1.\n",
    "'''\n",
    "def weight(s):\n",
    "\treturn tf.Variable(tf.truncated_normal(s, stddev=0.1))\n",
    "\n",
    "#Dimensionality of the hiddenLayer.  hiddenLayer should be [hiddenSize, 1]\n",
    "hiddenSize = _____\n",
    "\n",
    "'''\n",
    "Size should allign with the expected input and desired output\n",
    "Keep in mind a m x n matrix times a n x 1 vector results in a\n",
    "m x 1 vector.\n",
    "'''\n",
    "hiddenMap = weight(________)\n",
    "'''\n",
    "Define a variable initialized to a zero vector that can be\n",
    "added as shown below.\n",
    "\n",
    "Either search tf.Variable or see above for how you can use it.\n",
    "Hint: tf.zeros\n",
    "'''\n",
    "hiddenBias = tf.Variable(______)\n",
    "'''\n",
    "Find the tf function that performs the standard relu operation.\n",
    "Feel to free to use any other nonlinearity if you wish.\n",
    "'''\n",
    "hiddenLayer = ______(tf.matmul(hiddenMap, combinedState) + hiddenBias)\n",
    "\n",
    "'''\n",
    "predictionMap should be a weight with same dimensionality as\n",
    "the hidden layer.\n",
    "'''\n",
    "predictionMap = weight(______)\n",
    "'''\n",
    "The first blank should be filled with the tf function that\n",
    "accepts a tensor and reduces it to a single value, it's sum.\n",
    "\n",
    "The second blank should be filled with the tf function that\n",
    "computes the element wise product of the two given vectors.\n",
    "\n",
    "Note we are effectively computing the dot product here.\n",
    "'''\n",
    "logit = ______(______(predictionMap, hiddenLayer))\n",
    "'''\n",
    "Find a tf function that accepts any real number and outputs\n",
    "a number between 0 and 1.  We're looking for a specific activation\n",
    "function that allows our output to resemble our target values.\n",
    "'''\n",
    "prediction = _____(logit)\n",
    "\n",
    "'''\n",
    "Here we are computing the cross entropy loss.  epsilon is simply\n",
    "a small number that we add in our logarithms to avoid exploding losses.\n",
    "This is because log(tiny number) diverges to -infinity.\n",
    "\n",
    "Ignoring epsilon we want our loss to be as follows\n",
    "If y = 0.  Then the loss should be 0 when prediction = 0 and increase\n",
    "as prediction increases to 1.\n",
    "If y = 1.  Then the loss should be 0 when prediction = 1 and increase\n",
    "as prediction decreases to 0.\n",
    "\n",
    "Note the negative sign outside the whole expression.\n",
    "'''\n",
    "epsilon = 1e-2\n",
    "loss = -(____*tf.log(_____ + epsilon) + ____*tf.log(______ + epsilon))\n",
    "'''\n",
    "correct should be True/1 iff\n",
    "y = 0 and prediction is closer to 0 than 1\n",
    "or y = 1 and prediction is closer to 1 than 0\n",
    "Otherwise it should be False/0.\n",
    "'''\n",
    "correct = _____\n",
    "\n",
    "#Learning rate for our model, feel free to adjust between runs\n",
    "LEARNING_RATE = _____\n",
    "'''\n",
    "Find a tf function to minimize your loss.\n",
    "Feel free to use AdamOptimizer.\n",
    "'''\n",
    "trainStep = _____(LEARNING_RATE).minimize(loss)\n",
    "\n",
    "#Tensorflow session\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "#For saving/restoring previous weights\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "#Restore weights or initialize to new\n",
    "if(USE_OLD):\n",
    "\tprint(\"Restoring weights...\")\n",
    "\tsaver.restore(sess,\"saved/weights.ckpt\")\n",
    "else:\n",
    "\tsess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "No need to modify this.  Computes and prints accuracies.\n",
    "Positive accuracy is defined as the accuracy of the model\n",
    "on reviews that were actually positive and vice versa.\n",
    "'''\n",
    "def printPerformance(predictions):\n",
    "\tcorrectNegative = predictions[0][1]/(sum(predictions[0]))\n",
    "\tcorrectPositive = predictions[1][1]/(sum(predictions[1]))\n",
    "\tnormalizedAccuracy = 0.5*(correctPositive+correctNegative)\n",
    "\tprint(f\"Accuracies\")\n",
    "\tprint(f\"Positive Accuracy: {correctPositive} Negative Accuracy: {correctNegative}\")\n",
    "\tprint(f\"Overall Accuracy: {normalizedAccuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN\n",
    "if(MODE==0):\n",
    "\tprint(\"Training model...\")\n",
    "\n",
    "\t#Number of batches you want to train\n",
    "\tBATCHES = ____\n",
    "\t#Number of reviews you want to learn from in a batch\n",
    "\tBATCH_SIZE = ____\n",
    "\n",
    "\t'''\n",
    "\tWe're interested in normalized accuracy, that means\n",
    "\tthe average between positive and negative accuracy.\n",
    "\tHowever our training dataset is skewed with about 4 \n",
    "\ttimes as many positive as negative reviews.\n",
    "\n",
    "\tFor this reason directly sampling from the training\n",
    "\tdataset with no modification to the weight update\n",
    "\twill result in a model that is significantly biased\n",
    "\ttowards making positive predictions.\n",
    "\n",
    "\tThere are a few approaches to adressing this problem.\n",
    "\tPerhaps the most straightforward is to merly ensure\n",
    "\tthat during training the model is equally exposed to\n",
    "\tpositive and negative reviews.\n",
    "\n",
    "\tImplementation of this is up to you but make sure\n",
    "\tthat you are using the dataset to its fullest extent.\n",
    "\tThat is don't simply discard some data.\n",
    "\n",
    "\tRemember positiveTrain holds numPositive positive\n",
    "\treviews and negativeTrain holds numNegative negative\n",
    "\treviews.\n",
    "\n",
    "\tHere you may wish to initialize some variables to\n",
    "\thelp accomplish this.\n",
    "\t'''\n",
    "\n",
    "\t#CODE HERE\n",
    "\n",
    "\t#loop through batches\n",
    "\tfor i in range(BATCHES):\n",
    "\t\t#Cumulative loss for this batch\n",
    "\t\tbatchLoss = 0\n",
    "\t\t#List to remember predictions for this batch\n",
    "\t\t#[[False Positives, True Negatives], [False Negatives, True Positives]]\n",
    "\t\tpredictions = [[0,0],[0,0]]\n",
    "\t\tfor r in range(BATCH_SIZE):\n",
    "\t\t\tprint(f\"Batch {i+1} training {r+1}/{BATCH_SIZE}\", end='\\r')\n",
    "\t\t\t'''\n",
    "\t\t\tDepending on r, set the values of the variables 'title', 'text',\n",
    "\t\t\tand 'rating'.  This will be used as input to our model.\n",
    "\n",
    "\t\t\tHow you set these values will depend on how you wish the implement\n",
    "\t\t\tthe earlier mentioned task.\n",
    "\n",
    "\t\t\tYou may want to reference/update some variables you initialized\n",
    "\t\t\tin the previous 'CODE HERE' block.\n",
    "\t\t\t'''\n",
    "\t\t\t\n",
    "\t\t\t#CODE HERE\n",
    "\n",
    "\t\t\t#Train the model and compute the loss and whether or not the model was correct\n",
    "\t\t\t#Fill in the appropriate placeholder values\n",
    "\t\t\tl, c, _ = sess.run([loss, correct, trainStep], \n",
    "\t\t\t\t\t\t\tfeed_dict={titlePlaceholder:title, textPlaceholder:text, y:rating})\n",
    "\t\t\t#Increment the predictions and batchLoss as appropriate.\n",
    "\t\t\tpredictions[rating][c]+=1;\n",
    "\t\t\tbatchLoss+=l\n",
    "\t\tprint()\n",
    "\t\tprint(f\"Batch {i+1} Loss: {batchLoss/BATCH_SIZE}\")\n",
    "\t\tprintPerformance(predictions)\n",
    "\t\tprint(\"---------\")\n",
    "\t\t'''\n",
    "\t\tEvery SAVE_EVERY batches we compute the performance on the\n",
    "\t\tvalidation dataset and save the model weights.  Keep in mind\n",
    "\t\tthe purpose of the validation dataset.\n",
    "\t\t'''\n",
    "\t\tSAVE_EVERY = ____\n",
    "\t\tif(i%SAVE_EVERY==SAVE_EVERY-1):\n",
    "\t\t\tprint(\"Validation Performance\")\n",
    "\t\t\t#Similar tracking to actual training\n",
    "\t\t\tvalLoss = 0\n",
    "\t\t\tpredictions = [[0,0],[0,0]]\n",
    "\t\t\tfor v in valSet:\n",
    "\t\t\t\t#Get necessary information for a single validation review\n",
    "\t\t\t\ttitle, text, rating = v\n",
    "\t\t\t\t#Fill in the appropriate placeholder values\n",
    "\t\t\t\tl, c = sess.run([loss, correct], \n",
    "\t\t\t\t\t\t\tfeed_dict={titlePlaceholder:___, textPlaceholder:___, y:___})\n",
    "\t\t\t\tpredictions[rating][c]+=1;\n",
    "\t\t\t\tvalLoss+=l\n",
    "\t\t\tprint(f\"LOSS: {valLoss/numVal}\")\n",
    "\t\t\tprint(\"Saving weights...\")\n",
    "\t\t\tprintPerformance(predictions)\n",
    "\t\t\tsaver.save(sess, \"saved/weights.ckpt\")\n",
    "\t\tprint(\"---------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST with user input\n",
    "if(MODE==1):\n",
    "\twhile(True):\n",
    "\t\treviewTitle = input(\"Supply a Review Title: \")\n",
    "\t\treviewText = input(\"Supply text for your Review: \")\n",
    "\t\t'''\n",
    "\t\tFill in appropriate place holder values.  You might\n",
    "\t\thave to use an earlier function we wrote to parse\n",
    "\t\tthe user input.\n",
    "\t\t'''\n",
    "\t\tp = sess.run(prediction, feed_dict={titlePlaceholder:______, \n",
    "\t\t\t\t\t\t\t\t\t\t\ttextPlaceholder:______})\n",
    "\t\tif(p > 0.5):\n",
    "\t\t\tprint(f\"POSITIVE with {(p*100):.2f}% confidence.\")\n",
    "\t\telse:\n",
    "\t\t\tprint(f\"NEGATIVE with {((1-p)*100):.2f}% confidence.\")\n",
    "\t\tprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(MODE==2):\n",
    "\tprint(\"Making test predictions...\")\n",
    "\tpredictions = []\n",
    "\tfor i in range(numTest):\n",
    "\t\tprint(f\"Predicting {i+1}/{numTest}\",end='\\r')\n",
    "\t\ttitle, text = testSet[i]\n",
    "\t\t#fill in appropriate placeholder values\n",
    "\t\tpred = sess.run(prediction, feed_dict={titlePlaceholder:____, textPlaceholder:___})\n",
    "\t\t#Append binary model prediction to list\n",
    "\t\tpredictions.append(float(pred)>0.5)\n",
    "\t'''\n",
    "\tSave model prediction to appropriate json file\n",
    "\tRemember to write your name where requested.\n",
    "\tJust first name is fine.\n",
    "\t'''\n",
    "\tjson.dump(predictions, open('YOUR_NAME.json','w'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
